{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c65dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting serpapi\n",
      "  Downloading serpapi-0.1.5-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.10/site-packages (4.1.0)\n",
      "Collecting beautifulsoup4>=4.4.1 (from newspaper3k)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in ./.venv/lib/python3.10/site-packages (from newspaper3k) (11.2.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in ./.venv/lib/python3.10/site-packages (from newspaper3k) (6.0.2)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in ./.venv/lib/python3.10/site-packages (from newspaper3k) (5.4.0)\n",
      "Collecting nltk>=3.2.1 (from newspaper3k)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests>=2.10.0 in ./.venv/lib/python3.10/site-packages (from newspaper3k) (2.32.4)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.10/site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.4.1->newspaper3k)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (2025.6.15)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Downloading serpapi-0.1.5-py2.py3-none-any.whl (10 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k\n",
      "  Building wheel for tinysegmenter (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13634 sha256=3bfb9447ed1acd44c816aa2289cd6819fcf6c2ab633d947a7f2625a1b3c8d4d4\n",
      "  Stored in directory: /Users/marieernst/Library/Caches/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
      "  Building wheel for feedfinder2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3393 sha256=715cc5730176d5d8f1e7e9e1a24bd35ee25bb666a1d284f3dedc409af116bb17\n",
      "  Stored in directory: /Users/marieernst/Library/Caches/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
      "  Building wheel for jieba3k (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398402 sha256=decf6219bfb768f1ec1ad5560e34150e0e619bbdda711288ee9eb3bb33c2571f\n",
      "  Stored in directory: /Users/marieernst/Library/Caches/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, soupsieve, nltk, feedparser, cssselect, serpapi, requests-file, beautifulsoup4, tldextract, feedfinder2, newspaper3k\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13/13\u001b[0m [newspaper3k]\u001b[0m [newspaper3k]p4]\n",
      "\u001b[1A\u001b[2KSuccessfully installed beautifulsoup4-4.13.4 cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 nltk-3.9.1 requests-file-2.1.0 serpapi-0.1.5 sgmllib3k-1.0.0 soupsieve-2.7 tinysegmenter-0.3 tldextract-5.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k serpapi transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2eaf692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml_html_clean\n",
      "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.10/site-packages (from lxml_html_clean) (5.4.0)\n",
      "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: lxml_html_clean\n",
      "Successfully installed lxml_html_clean-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec97c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marieernst/Documents/Master/4. Semester/B5.3_Unternehmenssoftware/Greenwashing_Detector/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó https://www.volkswagen-group.com/de/pressemitteilungen/anteil-erneuerbarer-energien-bei-der-stromversorgung-der-werke-steigt-deutlich-16750\n",
      "‚ùå Keine klare Best√§tigung gefunden ‚Äì Score: 0.13\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "\n",
    "# 1. Setup Suchparameter (nutzt SerpAPI ‚Äì Google ohne Scraping)\n",
    "SERPAPI_KEY = \"ba9c850db9303f0b69b9e456775aef08905b12c33e3da2b4e1e273b6dd5597c8\"  # Kostenlos anmelden: serpapi.com\n",
    "QUERY = \"Volkswagen 100 Prozent √ñkostrom EU Produktion 2023\"\n",
    "VERSPRECHEN = \"Volkswagen nutzt seit 2023 in allen EU-Produktionsst√§tten 100 Prozent √ñkostrom.\"\n",
    "\n",
    "# 2. Lade lokales deutsches Sprachmodell\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased\")\n",
    "\n",
    "# 3. Suche relevante Nachrichten\n",
    "def suche_news(query, limit=5):\n",
    "    url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"tbm\": \"nws\",\n",
    "        \"api_key\": SERPAPI_KEY,\n",
    "        \"num\": limit,\n",
    "        \"hl\": \"de\",\n",
    "        \"gl\": \"de\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    ergebnisse = response.json().get(\"news_results\", [])\n",
    "    return [e[\"link\"] for e in ergebnisse]\n",
    "\n",
    "# 4. Artikel einlesen und Inhalte extrahieren\n",
    "def lade_artikel(url):\n",
    "    try:\n",
    "        artikel = Article(url, language=\"de\")\n",
    "        artikel.download()\n",
    "        artikel.parse()\n",
    "        return artikel.text\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden: {url} ‚Äì {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 5. √Ñhnlichkeit bewerten\n",
    "def pr√ºfe_versprechen(text, versprechen, threshold=0.7):\n",
    "    doc_embedding = model.encode(text, convert_to_tensor=True)\n",
    "    query_embedding = model.encode(versprechen, convert_to_tensor=True)\n",
    "    score = float(util.pytorch_cos_sim(doc_embedding, query_embedding))\n",
    "    return score >= threshold, score\n",
    "\n",
    "# 6. Hauptlogik\n",
    "links = suche_news(QUERY)\n",
    "for link in links:\n",
    "    print(f\"\\nüîó {link}\")\n",
    "    inhalt = lade_artikel(link)\n",
    "    if not inhalt:\n",
    "        continue\n",
    "    erf√ºllt, score = pr√ºfe_versprechen(inhalt, VERSPRECHEN)\n",
    "    if erf√ºllt:\n",
    "        print(f\"‚úÖ Versprechen scheint eingehalten laut Artikel ‚Äì Score: {score:.2f}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Keine klare Best√§tigung gefunden ‚Äì Score: {score:.2f}\")\n",
    "    time.sleep(1)  # API freundlich bleiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb9d56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó (1) https://www.volkswagen-group.com/de/pressemitteilungen/anteil-erneuerbarer-energien-bei-der-stromversorgung-der-werke-steigt-deutlich-16750\n",
      "üîé √Ñhnlichkeitsscore: 0.17\n",
      "\n",
      "‚ùå Keine Best√§tigung in den gepr√ºften Artikeln gefunden.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "\n",
    "#Konfiguration\n",
    "SERPAPI_KEY = \"ba9c850db9303f0b69b9e456775aef08905b12c33e3da2b4e1e273b6dd5597c8\"  \n",
    "QUERY = \"Volkswagen 100 Prozent √ñkostrom EU 2023\"\n",
    "VERSPRECHEN = \"Volkswagen nutzt ab 2023 in allen EU-Produktionsstandorten ausschlie√ülich √ñkostrom.\"\n",
    "MAX_ARTIKEL = 10\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "\n",
    "\n",
    "def suche_news(query, max_results=20):\n",
    "    url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"tbm\": \"nws\",\n",
    "        \"api_key\": SERPAPI_KEY,\n",
    "        \"num\": max_results,\n",
    "        \"hl\": \"de\",\n",
    "        \"gl\": \"de\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    ergebnisse = response.json().get(\"news_results\", [])\n",
    "    return [e[\"link\"] for e in ergebnisse]\n",
    "\n",
    "\n",
    "def lade_artikel(url):\n",
    "    try:\n",
    "        artikel = Article(url, language=\"de\")\n",
    "        artikel.download()\n",
    "        artikel.parse()\n",
    "        return artikel.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "#√Ñhnlichkeitspr√ºfung\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased\")\n",
    "\n",
    "def pr√ºfe_versprechen(text, versprechen, threshold=SIMILARITY_THRESHOLD):\n",
    "    doc_emb = model.encode(text, convert_to_tensor=True)\n",
    "    query_emb = model.encode(versprechen, convert_to_tensor=True)\n",
    "    score = float(util.pytorch_cos_sim(doc_emb, query_emb))\n",
    "    return score >= threshold, score\n",
    "\n",
    "\n",
    "def finde_bestaetigung(query, versprechen, max_artikel=10):\n",
    "    links = suche_news(query)\n",
    "    gepr√ºft = 0\n",
    "\n",
    "    for i, link in enumerate(links):\n",
    "        print(f\"\\nüîó ({i+1}) {link}\")\n",
    "        text = lade_artikel(link)\n",
    "\n",
    "        if len(text) < 500:\n",
    "            print(\"Artikeltext zu kurz ‚Äì √ºbersprungen.\")\n",
    "            continue\n",
    "\n",
    "        erf√ºllt, score = pr√ºfe_versprechen(text, versprechen)\n",
    "        print(f\"√Ñhnlichkeitsscore: {score:.2f}\")\n",
    "        gepr√ºft += 1\n",
    "\n",
    "        if erf√ºllt:\n",
    "            print(f\"Versprechen best√§tigt ‚Äì Score: {score:.2f}\")\n",
    "            return link\n",
    "\n",
    "        if gepr√ºft >= max_artikel:\n",
    "            print(\"Max. Artikelanzahl erreicht ‚Äì keine Best√§tigung gefunden.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # h√∂flich bleiben bei Anfragen\n",
    "\n",
    "    return None\n",
    "\n",
    "# ‚ñ∂Ô∏è Ausf√ºhrung\n",
    "link = finde_bestaetigung(QUERY, VERSPRECHEN)\n",
    "if link:\n",
    "    print(f\"\\n Best√§tigung gefunden: {link}\")\n",
    "else:\n",
    "    print(\"\\n Keine Best√§tigung in den gepr√ºften Artikeln gefunden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b03c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
