{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6068f3a8",
   "metadata": {},
   "source": [
    "# Vorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b517c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfb292",
   "metadata": {},
   "source": [
    "Text aus PDF-Dateien extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f02de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Lesen von {pdf_path}: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb128e42",
   "metadata": {},
   "source": [
    "Bereinigung des Texts von zusätzlichen Leerzeichen, Bindestrichen,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1593df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.replace('\\xad', '')\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfff245",
   "metadata": {},
   "source": [
    "Aufteilung des Textes in Sätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "effcf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    protected = {\n",
    "        r'\\bz\\. *B\\.': 'z.B.',\n",
    "        r'\\bu\\. *a\\.': 'u.a.',\n",
    "        r'\\bu\\. *Ä\\.': 'u.Ä.',\n",
    "        r'\\bd\\. *h\\.': 'd.h.',\n",
    "        r'\\bu\\. *s\\. *w\\.': 'usw.',\n",
    "    }\n",
    "    for pattern, replacement in protected.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-ZÄÖÜ])', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6f419",
   "metadata": {},
   "source": [
    "Gruppierung der Sätze in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34985f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sentences(sentences, max_length=500):\n",
    "    chunks, current = [], \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current) + len(sentence) + 1 <= max_length:\n",
    "            current += \" \" + sentence if current else sentence\n",
    "        else:\n",
    "            chunks.append(current.strip())\n",
    "            current = sentence\n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db947e",
   "metadata": {},
   "source": [
    "Kombination der Aufteilung von Sätzen und Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0537296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_chunk_text(text, max_length=500):\n",
    "    sentences = split_sentences(text)\n",
    "    return group_sentences(sentences, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea00ae",
   "metadata": {},
   "source": [
    "# LLM Modell\n",
    "\n",
    "Die Klasse definiert zunächst zentrale Parameter wie Host, Port, Timeout und Standardoptionen für die Modellgenerierung (z. B. Temperatur, Top-k, Stopbedingungen). Über die Methode models() kann geprüft werden, welche Modelle auf dem Server verfügbar sind. Mit pull_model() lassen sich Modelle bei Bedarf vom zentralen Ollama-Repository herunterladen und bereitstellen.\n",
    "\n",
    "Für die Textgenerierung stehen zwei Hauptfunktionen zur Verfügung:\n",
    "\n",
    "- completion() sendet einfache Prompts an das Modell.\n",
    "\n",
    "- chat() ermöglicht einen Austausch im Format eines Chatverlaufs mit mehreren Rollen (z. B. Nutzer und Modell).\n",
    "\n",
    "Beide Methoden nutzen intern api_request(), die die Kommunikation mit dem Ollama-Server übernimmt und dafür sorgt, dass Anfragen entweder als Streaming (Debugzwecken) oder klassisch per JSON beantwortet werden.\n",
    "\n",
    "Die Antwortverarbeitung erfolgt robust durch zwei weitere Methoden:\n",
    "\n",
    "- secure_text_response() extrahiert das Ergebnis aus der JSON-Antwort, berechnet die Dauer und die Anzahl generierter Tokens.\n",
    "\n",
    "- secure_json_response() prüft zusätzlich, ob die Antwort im JSON- oder Markdown-Format vorliegt und verarbeitet ggf. auch eingebettete <think>-Blöcke, falls das Modell seine Begründung mitschickt.\n",
    "\n",
    "Um sicherzustellen, dass fehlerhafte Kodierungen oder Zeichenfolgen nicht zu Problemen führen, wurde fix_invalid_escapes() ergänzt, die mithilfe der ftfy-Bibliothek Textprobleme korrigiert.\n",
    "\n",
    "Diese API-Klasse stellt damit eine flexible Schnittstelle dar, um eigene Texte (wie Aussagen aus Nachhaltigkeitsberichten) effizient mit lokalen Sprachmodellen zu analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63cad038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import ftfy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1695ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaApi:\n",
    "\n",
    "    HOST = \"https://f2ki-h100-1.f2.htw-berlin.de\"\n",
    "    PORT = 11435\n",
    "\n",
    "    TIMEOUT = 120\n",
    "    STREAM_RESPONSE = False\n",
    "\n",
    "    THINKING = False\n",
    "\n",
    "    FALSE_RETURN = {\"result\": None, \"time\": 0, \"token\": 0, \"info\": {}}\n",
    "\n",
    "    DEFAULT_OPTIONS = {\n",
    "        \"num_ctx\": 2048,        # Default: 2048\n",
    "        \"repeat_last_n\": 64,    # Default: 64, 0 = disabled, -1 = num_ctx\n",
    "        \"repeat_penalty\": 1.1,  # Default: 1.1\n",
    "        \"temperature\": 0.8,     # Default: 0.8\n",
    "        \"seed\": 0,              # Default: 0\n",
    "        \"stop\": [],             # No default\n",
    "        \"num_predict\": -1,      # Default: -1, infinite generation\n",
    "        \"top_k\": 40,            # Default: 40\n",
    "        \"top_p\": 0.9,           # Default: 0.9\n",
    "        \"min_p\": 0.0            # Default: 0.0\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_invalid_escapes(s):\n",
    "        if not isinstance(s, str):\n",
    "            return s\n",
    "\n",
    "        try:\n",
    "            s_re = ftfy.fix_text(s)\n",
    "            if s_re != s:\n",
    "                s = s_re\n",
    "        except Exception as e:\n",
    "            print(f\"Encoding failed for text: {e}\")\n",
    "\n",
    "        return s\n",
    "\n",
    "    @classmethod\n",
    "    def models(cls):\n",
    "        url = f\"{cls.HOST}:{cls.PORT}/api/tags\"\n",
    "        headers = {\n",
    "            \"accept\": \"application/json\",\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "            return False\n",
    "        else:\n",
    "            try:\n",
    "                json_data = response.json()\n",
    "                return json_data.get(\"models\", [])\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to parse JSON: {e}\")\n",
    "                return False\n",
    "\n",
    "    @classmethod\n",
    "    def pull_model(cls, name:str, tag:str):\n",
    "        url = f\"{cls.HOST}:{cls.PORT}/api/pull\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"accept\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": f\"{name}:{tag}\"\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, stream=True, timeout=cls.TIMEOUT)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Pull request failed with status {response.status_code}: {response.text}\")\n",
    "                return False\n",
    "\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        progress = json.loads(line)\n",
    "                        if 'status' in progress:\n",
    "                            print(f\"Pulling model: {progress['status']}\")\n",
    "                        if progress['status'] == \"success\":\n",
    "                            return True\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to pull model: {e}\")\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def completion(cls, prompt:str, model=\"phi4:latest\", schema=None, options=None):\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\" : prompt,\n",
    "            \"options\": {\n",
    "                **cls.DEFAULT_OPTIONS,\n",
    "                **options\n",
    "            } if options is not None else cls.DEFAULT_OPTIONS,\n",
    "        }\n",
    "        if schema is not None:\n",
    "            payload[\"format\"] = schema\n",
    "\n",
    "        return cls.api_request(payload, force_json=False if schema is None else True)\n",
    "\n",
    "    @classmethod\n",
    "    def chat(cls, chat, model=\"phi4:latest\", schema=None, options=None):\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": chat,\n",
    "            \"options\": {\n",
    "                **cls.DEFAULT_OPTIONS,\n",
    "                **options\n",
    "            } if options is not None else cls.DEFAULT_OPTIONS,\n",
    "        }\n",
    "        if schema is not None:\n",
    "            payload[\"format\"] = schema\n",
    "\n",
    "        return cls.api_request(payload, force_json=False if schema is None else True)\n",
    "\n",
    "    @classmethod\n",
    "    def api_request(cls, payload, force_json:bool):\n",
    "        if \"messages\" in payload:\n",
    "            # Chat Request\n",
    "            url = f\"{cls.HOST}:{cls.PORT}/api/chat\"\n",
    "        else:\n",
    "            # Completion Request\n",
    "            url = f\"{cls.HOST}:{cls.PORT}/api/generate\"\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"accept\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        payload = {\n",
    "            **payload,\n",
    "            \"think\": cls.THINKING,\n",
    "            \"stream\": cls.STREAM_RESPONSE,\n",
    "            \"keep_alive\": \"5m\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, stream=cls.STREAM_RESPONSE, timeout=cls.TIMEOUT)\n",
    "\n",
    "            if cls.STREAM_RESPONSE:\n",
    "                for line in response.iter_lines(decode_unicode=True):\n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "\n",
    "                        if 'message' in chunk and 'content' in chunk['message']:\n",
    "                            content = chunk['message']['content']\n",
    "                            print(content, end='', flush=True)\n",
    "\n",
    "                        if 'done' in chunk and chunk['done']:\n",
    "                            break\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"ERROR: Failed to decode JSON during streaming: {e}\")\n",
    "                return {**cls.FALSE_RETURN, \"info\": {\"error\": 'Streaming mode does not retreive a value'}}\n",
    "            else:\n",
    "                return cls.secure_json_response(response) if force_json else cls.secure_text_response(response)\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"ERROR: The request took to long. Adjust the timeout ({cls.TIMEOUT}) as needed\")\n",
    "            return {**cls.FALSE_RETURN, \"info\": {\"error\": f\"Request timeout ({cls.TIMEOUT}) reached\"}}\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Request exception: {e}\")\n",
    "            return {**cls.FALSE_RETURN, \"info\": {\"error\": f\"Request exception: {e}\"}}\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def secure_json_response(cls, response):\n",
    "\n",
    "        text_response = cls.secure_text_response(response)\n",
    "\n",
    "        if text_response.get(\"result\") is None:\n",
    "            return text_response\n",
    "\n",
    "        message = str(text_response.get(\"result\"))\n",
    "\n",
    "        markdown_response = False\n",
    "        thinking_block = False\n",
    "\n",
    "        if message.strip().startswith(\"<think>\"):\n",
    "            print('WARN: Model returned <think> reasoning block before JSON')\n",
    "            message = re.sub(r\"^\\s*<think>.*?</think>\\s*\", \"\", message, flags=re.DOTALL).strip()\n",
    "            thinking_block = True\n",
    "\n",
    "        match = re.search(r'```json(.*?)```', message, re.DOTALL)\n",
    "        if match:\n",
    "            # alles außer dem Inhalt von „```json“ bis „```“ entfernen\n",
    "            print('WARN: Model returned markdown instead of only JSON')\n",
    "            message = match.group(1).strip()\n",
    "            markdown_response = True\n",
    "\n",
    "        try:\n",
    "            # JSON parsen\n",
    "            result = json.loads(message)\n",
    "\n",
    "            # Überschreiben des Textergebnisses mit JSON dict\n",
    "            text_response[\"result\"] = dict(result)\n",
    "            text_response[\"info\"] = {\n",
    "                \"thinking\": thinking_block,\n",
    "                \"markdown\": markdown_response\n",
    "            }\n",
    "            return text_response\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"ERROR: Failed to decode JSON: {e}\")\n",
    "            return {**cls.FALSE_RETURN, \"info\": {\"error\": 'JSON decode error on the model\\'s response'}}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to parse JSON: {e}\")\n",
    "            return {**cls.FALSE_RETURN, \"info\": {\"error\": str(e)}}\n",
    "\n",
    "    @classmethod\n",
    "    def secure_text_response(cls, response):\n",
    "\n",
    "        try:\n",
    "            parsed_json = response.json()\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                err_msg = parsed_json.get('error', 'Unknown error')\n",
    "                print(f\"ERROR: Request failed with status {response.status_code}: {err_msg}\")\n",
    "                return {**cls.FALSE_RETURN, \"info\":{\"error\":err_msg}}\n",
    "\n",
    "            if 'done' not in parsed_json or parsed_json.get('done') is False:\n",
    "                print(\"ERROR: Response has returned but Model didn't complete the answer\")\n",
    "                return {**cls.FALSE_RETURN, \"info\": {\"error\": 'Incomplete answer'}}\n",
    "\n",
    "            # LLM Chat Rückgabe als String\n",
    "            message = parsed_json.get('message').get('content') if \"message\" in parsed_json else parsed_json.get('response')\n",
    "\n",
    "            microseconds_elapsed = parsed_json.get('total_duration')\n",
    "            seconds_elapsed = round(microseconds_elapsed / 1000000000, 3)\n",
    "            token_count = parsed_json.get('eval_count')\n",
    "\n",
    "            return {\n",
    "                \"result\": cls.fix_invalid_escapes(message),\n",
    "                \"time\": float(seconds_elapsed),\n",
    "                \"token\": int(token_count),\n",
    "                \"info\": {}\n",
    "            }\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"ERROR: Failed to decode JSON: {e}\")\n",
    "            return {**cls.FALSE_RETURN, \"info\": {\"error\": 'JSON decode error on the ollama server\\'s response'}}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to parse JSON: {e}\")\n",
    "            return {**cls.FALSE_RETURN, \"info\": {\"error\": str(e)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761eb61",
   "metadata": {},
   "source": [
    "# Filterung von Nachhaltigkeitsversprechen mit LLM\n",
    "\n",
    "Im folgenden Abschnitt wird der vollständige Workflow zur Analyse von PDF-Dokumenten (z. B. Nachhaltigkeits- oder Geschäftsberichte) beschrieben. Ziel ist es, mithilfe eines großen Sprachmodells relevante Nachhaltigkeitsversprechen aus dem Text zu extrahieren. Der Code ist in mehrere Teilfunktionen gegliedert, um Wiederverwendbarkeit und Übersichtlichkeit zu gewährleisten.\n",
    "\n",
    "Für die spätere Klassifikation sollten die Nachhaltigkeitsberichte folgende Benennung erhalten: Unternehmen_Jahr.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9939a",
   "metadata": {},
   "source": [
    "Modell und Prompt wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71219e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Using model: llama3.1:70b\n"
     ]
    }
   ],
   "source": [
    "# Modell wählen\n",
    "model_name = \"llama3.1:70b\"\n",
    "print(f\"-------\\nUsing model: {model_name}\")\n",
    "\n",
    "# Ordnerpfad mit PDFs\n",
    "pdf_folder_path = \"./mercedes\"\n",
    "\n",
    "# Prompt-Vorlage\n",
    "prompt_template = \"\"\"\n",
    "\"Extrahiere bitte alle Nachhaltigkeitsversprechen, die ein konkretes Ziel verfolgen, aus folgendem Text. Gib nur den relevanten Text zurück, keine Einleitung oder sonstigen Text:\\n\",\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f0b4a",
   "metadata": {},
   "source": [
    "Analyse der PDF-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d2ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF-Dateien durchgehen\n",
    "def is_promise(text):\n",
    "    negativ_phrase = [\n",
    "        \"keine\", \"nicht enthalten\", \"nicht vorhanden\", \"leider\", \"keine Angaben\",\n",
    "        \"keine relevanten\", \"keine Informationen\", \"keine konkreten\", \"keine expliziten\",\n",
    "        \"keine spezifischen Ziele\", \"nur Begriffserklärungen\", \"keine Nachhaltigkeitsversprechen\",\n",
    "    ]\n",
    "    return not any(phrase.lower() in text.lower() for phrase in negativ_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bb3d575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Verarbeite Datei: Mercedes-Benz_2023.pdf\n",
      "PDF in 1979 Chunks aufgeteilt.\n",
      "\n",
      "→ Chunk 1/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 2/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 3/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 4/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 5/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 6/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 7/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 8/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 9/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 10/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 11/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 12/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 13/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 14/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 15/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 16/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 17/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 18/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 19/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 20/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 21/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 22/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 23/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 24/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 25/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 26/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 27/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 28/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 29/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 30/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 31/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 32/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 33/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 34/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 35/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 36/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 37/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 38/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 39/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 40/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 41/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 42/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 43/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 44/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 45/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 46/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 47/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 48/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 49/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 50/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 51/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 52/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 53/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 54/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 55/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 56/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 57/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 58/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 59/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 60/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 61/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 62/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 63/1979 wird analysiert...\n",
      "\n",
      "→ Chunk 64/1979 wird analysiert...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mchunk)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mOllamaApi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     extracted \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extracted:\n",
      "Cell \u001b[0;32mIn[18], line 104\u001b[0m, in \u001b[0;36mOllamaApi.completion\u001b[0;34m(cls, prompt, model, schema, options)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m schema\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 143\u001b[0m, in \u001b[0;36mOllamaApi.api_request\u001b[0;34m(cls, payload, force_json)\u001b[0m\n\u001b[1;32m    135\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload,\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthink\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mTHINKING,\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mSTREAM_RESPONSE,\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5m\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m }\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTREAM_RESPONSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mSTREAM_RESPONSE:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines(decode_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/Master/4. Semester/B5.3_Unternehmenssoftware/Greenwashing_Detector/.venv/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master/4. Semester/B5.3_Unternehmenssoftware/Greenwashing_Detector/.venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master/4. Semester/B5.3_Unternehmenssoftware/Greenwashing_Detector/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Documents/Master/4. Semester/B5.3_Unternehmenssoftware/Greenwashing_Detector/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Documents/Master/4. Semester/B5.3_Unternehmenssoftware/Greenwashing_Detector/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Documents/Master/4. Semester/B5.3_Unternehmenssoftware/Greenwashing_Detector/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Master/4. Semester/B5.3_Unternehmenssoftware/Greenwashing_Detector/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Documents/Master/4. Semester/B5.3_Unternehmenssoftware/Greenwashing_Detector/.venv/lib/python3.10/site-packages/urllib3/connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        full_path = os.path.join(pdf_folder_path, filename)\n",
    "        print(f\"\\n---\\nVerarbeite Datei: {filename}\")\n",
    "\n",
    "        # Text extrahieren und bereinigen\n",
    "        raw_text = extract_text_from_pdf(full_path)\n",
    "        if not raw_text.strip():\n",
    "            print(\"PDF enthält keinen extrahierbaren Text.\")\n",
    "            continue\n",
    "\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        chunks = split_and_chunk_text(cleaned_text, max_length=500)\n",
    "        print(f\"PDF in {len(chunks)} Chunks aufgeteilt.\")\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"\\n→ Chunk {i+1}/{len(chunks)} wird analysiert...\")\n",
    "            prompt = prompt_template.format(text=chunk)\n",
    "            try:\n",
    "                result = OllamaApi.completion(prompt, model=model_name)\n",
    "                extracted = result.get(\"result\", \"\").strip()\n",
    "                if extracted:\n",
    "                    all_results.append(extracted)\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei der Modellabfrage (Chunk {i+1}): {e}\")\n",
    "\n",
    "        # Speichern extrahierter Inhalte zur Weiterverarbeitung\n",
    "        promises = [res for res in all_results if is_promise(res)]\n",
    "        \n",
    "        with open(f\"results_LLM/extracted_{filename}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for res in promises:\n",
    "                f.write(res + \"\\n\")\n",
    "        # Ergebnisse anzeigen\n",
    "        print(f\"\\n### Extrahierte Nachhaltigkeitsversprechen aus {filename}:\\n\")\n",
    "        for i, res in enumerate(all_results):\n",
    "            print(f\"--- Chunk {i+1} ---\\n{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2daeb1",
   "metadata": {},
   "source": [
    "# Überprüfung der Einhaltung der Versprechen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c50793",
   "metadata": {},
   "source": [
    "Die Überprüfung der Nachhaltigkeitsversprechen wurde mit ChatGPT durchgeführt, da OllamaApi keinen Internetzugriff besitzt und nicht zur Suche nach Artikeln geeignet ist. Es wurde auch eine Artikelsuche mittels Serpapi versucht, jedoch wurden hiermit keine guten Ziele erreicht.\n",
    "\n",
    "Damit die Liste an Nachhaltigkeitsverprechen ChatGPT übergeben werden können musste die Liste nochmals kompakter formuliert werden. Hierfür wurde die zuvor generierte Liste an Nachhaltigkeitsversrechen noch einmal der OllamaApi übergeben. Hierdurch wird die Liste gekürzt, indem doppelte Versprechen oder nicht klar definierte Ziele aussortiert werden. Es werden alle Nachhatligkeitsversprechen dem Modell auf einmal übergben. Anschließend kann die enstandene Liste an Versprechen mit ChatGPT überprüft werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "947fe636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Verarbeite Datei: extracted_Audi_2023.pdf.txt\n",
      "→ Extraktion erfolgreich.\n",
      "→ Ergebnisse gespeichert in: results_promises_chatgpt_format/prepared_Audi_2023.txt\n",
      "\n",
      "→ Verarbeite Datei: extracted_Porsche_2023.pdf.txt\n",
      "→ Extraktion erfolgreich.\n",
      "→ Ergebnisse gespeichert in: results_promises_chatgpt_format/prepared_Porsche_2023.txt\n",
      "\n",
      " Alle Dateien wurden verarbeitet.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Modell wählen\n",
    "model_name = \"llama3.1:70b\"\n",
    "\n",
    "# Prompt-Vorlage\n",
    "prompt_template = \"\"\"\n",
    "Du bist ein Textanalyse-Tool, das Nachhaltigkeitsversprechen aus Unternehmensdokumenten extrahiert.\n",
    "\n",
    "Analysiere den folgenden Text und extrahiere **alle Nachhaltigkeitsversprechen** in **klaren, prägnanten Stichpunkten**.\n",
    "\n",
    "**Wichtige Regeln:**\n",
    "- Entferne alle doppelten oder sinngemäß gleichen Aussagen (Deduplizierung).\n",
    "- Formuliere jeden Punkt neutral und sachlich.\n",
    "- Vermeide Wiederholungen oder Variationen derselben Aussage.\n",
    "- Achte auf konkrete Absichten, Maßnahmen, Ziele oder Prinzipien im Bereich Umwelt, Soziales oder Unternehmensführung (ESG).\n",
    "- Gib nur die Versprechen wieder – keine allgemeinen Aussagen oder Beschreibungen.\n",
    "\n",
    "**Format:**\n",
    "- Gib die Ergebnisse in einer nummerierten Liste zurück.\n",
    "- Max. ein Satz pro Stichpunkt.\n",
    "\n",
    "**Text:**  \n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Eingabe- und Ausgabeordner\n",
    "input_folder = \"results_LLM\"\n",
    "output_folder = \"results_promises_chatgpt_format\"\n",
    "\n",
    "# Sicherstellen, dass der Ausgabeordner existiert\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Durchlaufe alle .txt-Dateien im Ordner\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Dateiinhalt lesen\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_content = file.read()\n",
    "\n",
    "        # Prompt erstellen\n",
    "        prompt = prompt_template.format(text=text_content)\n",
    "        print(f\"\\n→ Verarbeite Datei: {filename}\")\n",
    "\n",
    "        try:\n",
    "            result = OllamaApi.completion(prompt, model=model_name)\n",
    "            extracted = result.get(\"result\", \"\").strip()\n",
    "\n",
    "            if extracted:\n",
    "                print(f\"→ Extraktion erfolgreich.\")\n",
    "                # Jede Zeile des Ergebnisses wird einzeln in CSV geschrieben\n",
    "                extracted_lines = [line.strip() for line in extracted.split(\"\\n\") if line.strip()]\n",
    "            else:\n",
    "                print(\"→ Keine Ergebnisse extrahiert.\")\n",
    "                extracted_lines = [\"(keine Versprechen extrahiert)\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei Datei {filename}: {e}\")\n",
    "            extracted_lines = [\"Fehler bei Modellabfrage\"]\n",
    "\n",
    "        # Ausgabedateiname erzeugen: prepared_<Marke>_<Jahr>.txt\n",
    "        base_name = os.path.splitext(filename)[0]  # entfernt .txt\n",
    "        parts = base_name.replace(\"extracted_\", \"\").replace(\".pdf\", \"\").split(\"_\")\n",
    "        if len(parts) >= 2:\n",
    "            brand = parts[0]\n",
    "            year = parts[1]\n",
    "            txt_name = f\"prepared_{brand}_{year}.txt\"\n",
    "        else:\n",
    "            txt_name = f\"prepared_unbekannt.txt\"\n",
    "\n",
    "        txt_path = os.path.join(output_folder, txt_name)\n",
    "\n",
    "        # Ergebnisse in TXT-Datei schreiben\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as txtfile:\n",
    "            txtfile.write(extracted)\n",
    "\n",
    "        print(f\"→ Ergebnisse gespeichert in: {txt_path}\")\n",
    "\n",
    "print(\"\\n Alle Dateien wurden verarbeitet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556a761",
   "metadata": {},
   "source": [
    "## Prompting Überprüfung Versprechen\n",
    "\n",
    "Der Prompt zur Überprüfung der Nachhaltigkeitsversprechen sieht wie folgt aus:\n",
    "\n",
    "**Rolle:**  \n",
    "Du bist ein kritischer Nachhaltigkeitsanalyst mit Fokus auf ESG-Zielerreichung großer Unternehmen.\n",
    "\n",
    "**Aufgabe:**  \n",
    "Bewerte eine Liste von Nachhaltigkeitsversprechen aus dem VW-Nachhaltigkeitsbericht 2023 hinsichtlich ihrer Einhaltung und Realisierbarkeit.\n",
    "\n",
    "**Anleitung:**  \n",
    "Für jedes aufgeführte Versprechen führe bitte die folgende Analyse durch:\n",
    "\n",
    "1. **Statusbewertung**: Wurde das Versprechen bereits\n",
    "   - vollständig erfüllt,\n",
    "   - teilweise erfüllt,\n",
    "   - nicht erfüllt,\n",
    "   oder\n",
    "   - liegt die Zielerreichung noch in der Zukunft?\n",
    "\n",
    "2. **Prognosebewertung (nur falls Zukunftsziel)**: \n",
    "   - Wie realistisch ist es, dass VW dieses Ziel in der angestrebten Zeit erreichen wird? (realistisch / unsicher / unrealistisch)\n",
    "\n",
    "3. **Beleg**: Nutze vertrauenswürdige Quellen (z. B. Zeitungsartikel, ESG-Ratings, Reports von NGOs oder Wirtschaftsanalysten), die **nach 2023 veröffentlicht** wurden. Verwende **keine** Veröffentlichungen der Volkswagen Group oder Volkswagen. Beziehe dich **nur** auf **unabhängige Quellen**. Verlinke jede Quelle direkt.\n",
    "\n",
    "4. **Ampelstatus**:\n",
    "   - 🟢 **Grün**: erfüllt oder realistisch\n",
    "   - 🟡 **Gelb**: teilweise erfüllt oder unsicher\n",
    "   - 🔴 **Rot**: nicht erfüllt oder unrealistisch\n",
    "\n",
    "**Format der Antwort:**  \n",
    "Gib die Ergebnisse bitte in folgender Tabellenstruktur zurück:\n",
    "\n",
    "| # | Nachhaltigkeitsversprechen | Status | Prognose (falls zutreffend) | Quelle(n) | Ampel |\n",
    "|---|-----------------------------|--------|------------------------------|-----------|-------|\n",
    "| 1 | [Versprechen 1]             | ...    | ...                          | ...       | ...   |\n",
    "| 2 | [Versprechen 2]             | ...    | ...                          | ...       | ...   |\n",
    "\n",
    "**Zu überprüfende Liste:**  \n",
    "[extrahierte Versprechen]\n",
    "\n",
    "Alle Ergebnisse sind im Ordner \"results_chatgpt\" zu finden\n",
    "\n",
    "## Beispielergebnis VW:\n",
    "| # | Nachhaltigkeitsversprechen                                                                                      | Status                                                                                                                                                        | Prognose (falls zutreffend) | Quelle(n)                                                                                                                                                                                                            | Ampel |\n",
    "| - | --------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- |\n",
    "| 1 | **80 % der direkten Lieferanten mit ISO 14001 oder EMAS (Ziel 2023)**                                           | ✅ teilweise erfüllt – VW meldet 80 %, unabhängig nicht verifiziert                                                                                            | –                           | VW-Bericht für Basisjahr 2023 (Quelle intern) – unabhängige Bestätigung fehlt                                                                                                                                        | 🟡    |\n",
    "| 2 | **85 % der direkten Lieferanten mit ISO 14001 oder EMAS (Ziel 2022)**                                           | ✅ teilweise erfüllt – im Report 2023 wird Anteil mit 85 % angegeben                                                                                           | –                           | wie oben (kein unabhängiger Nachweis gefunden)                                                                                                                                                                       | 🟡    |\n",
    "| 3 | **Neue Lieferanten mit sozial‑/umweltbewertung: 26 % (2023), 12 % (2022)**                                      | ❌ ungeprüft – keine unabhängige Quelle zur validen Bestätigung dieser Werte                                                                                   | –                           | keine unabhängige Datenerhebung verfügbar                                                                                                                                                                            | 🔴    |\n",
    "| 4 | **Alle Lieferanten müssen VW‑Nachhaltigkeitsanforderungen erfüllen (A‑ oder B‑Rating vergabefähig)**            | ✅ teilweise – laut Amnesty-Ranking VW nur mit 41/90 Punkten bewertet → viele Defizite in HR-DD                                                                | –                           | Amnesty „Recharge for Rights“ EV‑Ranking 2024 – VW erzielt Score 41/90([Volkswagen Group Annual Report 2024][1], [Industry Intelligence Inc.][2], [Reddit][3])                                                       | 🟡    |\n",
    "| 5 | **Transparenz in Lieferkette Batterierohstoffe (Kobalt, Lithium, Nickel, Graphit)**                             | ✅ teilweise – VW intern umfangreich aktiv; jedoch laut Amnesty und NGOs weiterhin erhebliche Defizite in Transparenz                                          | –                           | Amnesty‑Report und Analysen zeigen VW sehr schwache Offenlegung/HR‑Due‑Diligence([WIRED][4], [Reddit][3]); Leipzig-Studie kritisiert weiterhin ungenannte Schmelzer, Menschenrechtsrisiken([Universität Leipzig][5]) | 🟡    |\n",
    "| 6 | **OECD Due Diligence Guidance („Due Diligence Guidance for Responsible Supply Chains of Minerals…“) Umsetzung** | ✅ teilweise – VW adressiert Richtlinien, aber Amnesty-Ranking kritisiert weiterhin oberflächliche Ansätze                                                     | –                           | Amnesty 2024 bewertet VW weiterhin schlecht in HR-DD([Reddit][3], [WIRED][4]); VW-Analysen zeigen Fortschritt, aber kaum Konsequenzen bei Verstößen([Universität Leipzig][5], [Volkswagen Group][6])                 | 🟡    |\n",
    "| 7 | **Engagement zum Schutz risikogefährdeter Gruppen entlang Lieferkette**                                         | ❌ nicht erfüllt – VW bekam schlechte Bewertung bei Menschenrechts-DD, auch 2024 weiter als \"industry worst appraisal\" eingestuft                              | –                           | Amnesty-Ranking: VW Score 41/90 zeigt schwaches Engagement für Schutz gefährdeter Gruppen([Reddit][3], [WIRED][4])                                                                                                   | 🔴    |\n",
    "| 8 | **Verbesserung des sozialen Wohlergehens und Teilnahme an Gesundheits‑ und Sicherheitsmaßnahmen vor Ort**       | ✅ teilweise erfüllt – VW beteiligt sich an GIZ‑Initiative „Cobalt for Development“, aber unabhängige Berichte sehen begrenzten Impact                         | –                           | Leipzig/Afrikastudien analysieren VW‑Projekte kritisch: mögliche Greenwashing‑Risiken, begrenzte direkte Wirkung vor Ort([Universität Leipzig][5])                                                                   | 🟡    |\n",
    "| 9 | **Dekarbonisierung in der Lieferkette vorantreiben**                                                            | ✅ teilweise – VW fordert CO₂‑Ziele bei Lieferanten und Pilotprojekte (z. B. Recycling), aber keine unabhängige Bewertung über tatsächliche Emissionsreduktion | –                           | VW interne Angaben zu CO₂‑Verträgen und Recyclingpilot, jedoch keine unabhängigen Studien zur Wirkung verfügbar                                                                                                      | 🟡    |\n",
    "\n",
    "[1]: https://annualreport2024.volkswagen-group.com/sustainability-report/environment/introduction-to-environmental-management.html?utm_source=chatgpt.com \"Introduction to environmental management - Volkswagen Group Annual Report 2024\"\n",
    "[2]: https://www.industryintel.com/news/vw-joins-initiative-for-responsible-mining-assurance-will-gradually-apply-irma-standards-to-its-battery-supply-chains-for-raw-materials-such-as-cobalt-lithium-nickel-graphite-irma-established-rigorous-standards-for-responsible-raw-material-extraction-157018415640?utm_source=chatgpt.com \"VW joins Initiative for Responsible Mining Assurance, will gradually apply IRMA standards to its battery supply chains for raw materials such as cobalt, lithium, nickel, graphite; IRMA established rigorous standards for responsible raw material extraction\"\n",
    "[3]: https://www.reddit.com/r/EuroEV/comments/1g6dngo?utm_source=chatgpt.com \"Amnesty International: New human rights ranking of electric vehicle industry exposes laggards\"\n",
    "[4]: https://www.wired.com/story/the-worlds-biggest-maker-of-evs-has-the-worst-appraisal-of-human-rights?utm_source=chatgpt.com \"The World's Biggest Maker of EVs Has the Worst Appraisal of Human Rights\"\n",
    "[5]: https://home.uni-leipzig.de/~afrikastudien/2022/02/17/global-energy-justice-the-case-of-cobalt/?utm_source=chatgpt.com \"Global Energy Justice: The Case of Cobalt – Afrikastudien\"\n",
    "[6]: https://www.volkswagen-group.com/en/press-releases/volkswagen-group-publishes-third-responsible-raw-materials-report-17465?utm_source=chatgpt.com \"Volkswagen Group publishes third Responsible Raw Materials Report | Volkswagen Group\"\n",
    "\n",
    "\n",
    "Da für Punkt 3 keine unabhängigen Quellen gefunden wurden wurde eine Volkswagen Veröffentlichung als Gundlage für die Einschätzung verwendet:\n",
    "| #  | Nachhaltigkeitsversprechen                                                           | Status                                                                                                                                                                       | Prognose (falls Zukunftsziel) | Quelle(n)                                                                                                                                                                                                                  | Ampel |\n",
    "| -- | ------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- |\n",
    "| 3  | Anteil neuer Lieferanten mit Sozial‑/ Umweltbewertung: 26 % (2023) bzw. 12 % (2022)  | Ziel war zahlenbasiert (Ist‑Wert): tatsächlich 26  % 2023 (korrekt)                                                                                                          | –                             | Daten aus VW‑Bericht 2023; 2024 keine weitere Zielwert‑Angabe, aber Basis bestätigt ([Volkswagen Group Annual Report 2024][1], [VW Annual Report 2023][3])                                                                 | 🟢    |\n",
    "\n",
    "[3]: https://annualreport2023.volkswagen-group.com/group-management-report/sustainable-value-enhancement/procurement.html?utm_source=chatgpt.com \"Procurement - VW Annual Report 2023\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe27b15",
   "metadata": {},
   "source": [
    "# Zero-Shot-Klassifikation von Nachhaltigkeitsaussagen mit DeBERTa aus den Nachhaltigkeitsberichten\n",
    "\n",
    "Der Code nutzt ein mehrsprachiges Zero-Shot-Klassifikationsmodell („MoritzLaurer/mDeBERTa-v3-base-mnli-xnli“), um Textabschnitte (Chunks) aus Nachhaltigkeitsberichten automatisch in vorgegebene Kategorien wie „konkrete Maßnahme“, „vage Behauptung“ oder „fragwürdige Nachhaltigkeitsaussage“ einzuordnen.\n",
    "\n",
    "Dabei kommt die DeBERTa-Variante mit MNLI/XNLI-Feintuning zum Einsatz, die besonders gut für Klassifikationen in verschiedenen Sprachen geeignet ist. Für jede Aussage wird die wahrscheinlichste Kategorie bestimmt. Ist die Klassifikation unsicher – etwa bei einem niedrigen Score (< 0.5) oder einer geringen Differenz zwischen den Top-Labels (< 0.1) – wird die Aussage zusätzlich mit dem Label „unsicher“ versehen.\n",
    "\n",
    "Der Ansatz eignet sich insbesondere für große Textmengen aus LLM-generierten Extraktionen und ermöglicht eine erste strukturierte Einordnung von Nachhaltigkeitsversprechen ohne manuelles Labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bd82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dokumente\\Uni Master\\SoSe25\\Unternehmenssoftware\\Projekt\\Greenwashing_Detector\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84b344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dokumente\\Uni Master\\SoSe25\\Unternehmenssoftware\\Projekt\\Greenwashing_Detector\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\schmu\\.cache\\huggingface\\hub\\models--MoritzLaurer--mDeBERTa-v3-base-mnli-xnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Klassifiziere Aussagen für: Audi, 2023 (305 Aussagen)\n",
      "Ergebnisse gespeichert in: ./results_classification\\Audi_2023.csv\n",
      "label\n",
      "konkrete Maßnahme                     241\n",
      "unsicher                               55\n",
      "fragwürdige Nachhaltigkeitsaussage      5\n",
      "vage Behauptung                         4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "→ Klassifiziere Aussagen für: Porsche, 2023 (515 Aussagen)\n",
      "Ergebnisse gespeichert in: ./results_classification\\Porsche_2023.csv\n",
      "label\n",
      "konkrete Maßnahme                     405\n",
      "unsicher                               80\n",
      "vage Behauptung                        18\n",
      "fragwürdige Nachhaltigkeitsaussage      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "→ Klassifiziere Aussagen für: VolkswagenGroup, 2023 (519 Aussagen)\n",
      "Ergebnisse gespeichert in: ./results_classification\\VolkswagenGroup_2023.csv\n",
      "label\n",
      "konkrete Maßnahme                     443\n",
      "unsicher                               60\n",
      "fragwürdige Nachhaltigkeitsaussage      9\n",
      "vage Behauptung                         7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")\n",
    "\n",
    "# Klassifikationslabels\n",
    "labels = [\n",
    "    \"konkrete Maßnahme\",\n",
    "    \"vage Behauptung\",\n",
    "    \"fragwürdige Nachhaltigkeitsaussage\"\n",
    "]\n",
    "\n",
    "input_folder = \"./results_LLM\"\n",
    "output_folder = \"./results_classification\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.startswith(\"extracted_\") and filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "        if not lines:\n",
    "            continue\n",
    "\n",
    "        match = re.match(r\"extracted_(.+)_(\\d{4})\\.pdf\\.txt\", filename)\n",
    "        if not match:\n",
    "            print(f\"Dateiname nicht im erwarteten Format: {filename}\")\n",
    "            continue\n",
    "\n",
    "        company = match.group(1).replace(\"_\", \" \")\n",
    "        year = match.group(2)\n",
    "        print(f\"\\n→ Klassifiziere Aussagen für: {company}, {year} ({len(lines)} Aussagen)\")\n",
    "\n",
    "        classification_results = []\n",
    "\n",
    "        filtered_lines = [\n",
    "            line for line in lines\n",
    "            if not line.lower().startswith(\"ich bin bereit\") and \"füge den text hinzu\" not in line.lower()\n",
    "        ]\n",
    "\n",
    "        for chunk in filtered_lines:\n",
    "            try:\n",
    "                result = classifier(\n",
    "                    chunk,\n",
    "                    candidate_labels=labels,\n",
    "                    hypothesis_template=\"Diese Aussage stellt eine {} im Kontext von Nachhaltigkeit dar.\"\n",
    "                )\n",
    "\n",
    "                top_label = result[\"labels\"][0]\n",
    "                top_score = result[\"scores\"][0]\n",
    "                second_score = result[\"scores\"][1]\n",
    "                score_diff = top_score - second_score\n",
    "\n",
    "                # Unsichere Klassifikation kennzeichnen\n",
    "                if top_score < 0.5 or score_diff < 0.1:\n",
    "                    label = \"unsicher\"\n",
    "                else:\n",
    "                    label = top_label\n",
    "\n",
    "                classification_results.append({\n",
    "                    \"company\": company,\n",
    "                    \"year\": year,\n",
    "                    \"text\": chunk,\n",
    "                    \"label\": label,\n",
    "                    \"score\": top_score,\n",
    "                    \"score_diff\": score_diff,\n",
    "                    \"all_labels\": result[\"labels\"],\n",
    "                    \"all_scores\": result[\"scores\"]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei der Klassifikation: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Als CSV speichern\n",
    "        df = pd.DataFrame(classification_results)\n",
    "        df[\"text_short\"] = df[\"text\"].str.slice(0, 150)\n",
    "\n",
    "        csv_filename = f\"{company}_{year}.csv\".replace(\" \", \"_\")\n",
    "        output_path = os.path.join(output_folder, csv_filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Ergebnisse gespeichert in: {output_path}\")\n",
    "        print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36458c9b",
   "metadata": {},
   "source": [
    "# Evaluation der Klassifikation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ac4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
